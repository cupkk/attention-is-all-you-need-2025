import sentencepiece as spm
import os
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def train_bpe(input_file, model_prefix, vocab_size=32000):
    """
    è®­ç»ƒ BPE åˆ†è¯å™¨
    
    å‚æ•°:
        input_file: è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€å¥ï¼‰
        model_prefix: è¾“å‡ºæ¨¡å‹å‰ç¼€ï¼ˆå¦‚ bpe_deï¼‰
        vocab_size: è¯è¡¨å¤§å°ï¼ˆå»ºè®® 32000ï¼‰
    """
    try:
        spm.SentencePieceTrainer.train(
            input=input_file,
            model_prefix=model_prefix,
            vocab_size=vocab_size,
            model_type="bpe",
            pad_id=0, unk_id=1, bos_id=2, eos_id=3,
            character_coverage=1.0,
            max_sentence_length=4192000
        )
        logger.info(f"âœ… BPE åˆ†è¯å™¨å·²è®­ç»ƒå®Œæˆï¼š{model_prefix}.model")
    except Exception as e:
        logger.error(f"BPE åˆ†è¯å™¨è®­ç»ƒå¤±è´¥: {str(e)}")
        raise

def merge_train_files(train_files, output_path):
    """
    åˆå¹¶å¤šä¸ªè®­ç»ƒæ–‡ä»¶åˆ°ä¸€ä¸ªæ–‡ä»¶
    
    å‚æ•°:
        train_files: è®­ç»ƒæ–‡ä»¶åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with open(output_path, "w", encoding="utf-8") as fout:
        for fname in train_files:
            file_path = os.path.join(cleaned_dir, fname)
            if not os.path.exists(file_path):
                logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            logger.info(f"Reading {fname} ...")
            with open(file_path, "r", encoding="utf-8") as fin:
                fout.write(fin.read())
    logger.info(f"ğŸ“š å·²åˆå¹¶æ‰€æœ‰è®­ç»ƒæ•°æ®åˆ° {output_path}")

if __name__ == "__main__":
    cleaned_dir = "cleaned_data"
    bpe_dir = "bpe_models"
    os.makedirs(bpe_dir, exist_ok=True)

    # åˆå¹¶æ‰€æœ‰è®­ç»ƒæ•°æ®ä½œä¸º BPE æ¨¡å‹è®­ç»ƒè¯­æ–™
    all_train_path = os.path.join(cleaned_dir, "all_train.txt")

    # æ”¯æŒå¤šä¸ªè®­ç»ƒæ–‡ä»¶
    train_files = [
        "train_part1.de.cleaned",
        "train_part1.en.cleaned",
        "validation.de.cleaned",
        "validation.en.cleaned",
        "test.de.cleaned",
        "test.en.cleaned"
    ]

    merge_train_files(train_files, all_train_path)

    # è®­ç»ƒå¾·è¯­ BPE åˆ†è¯å™¨
    train_bpe(
        input_file=all_train_path,
        model_prefix=os.path.join(bpe_dir, "bpe_de"),
        vocab_size=32000
    )

    # è®­ç»ƒè‹±è¯­ BPE åˆ†è¯å™¨
    train_bpe(
        input_file=all_train_path,
        model_prefix=os.path.join(bpe_dir, "bpe_en"),
        vocab_size=32000
    )

    logger.info("âœ… BPE åˆ†è¯å™¨è®­ç»ƒå®Œæˆï¼Œè·¯å¾„:", bpe_dir)