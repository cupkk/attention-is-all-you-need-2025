import os
import math
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard.writer import SummaryWriter
from models.Encoder import Encoder
from models.Decoder import Decoder
from utils.mask import create_mask
from nltk.translate.bleu_score import corpus_bleu
from torch.nn.utils.clip_grad import clip_grad_norm
# ä¿®å¤AMPå¯¼å…¥ - ä½¿ç”¨æ­£ç¡®çš„API
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()


# å›ºå®šéšæœºç§å­
import random
random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

# 1. åŠ è½½è¯è¡¨
vocab_src = torch.load("multi30k_processed_bpe/vocab_de.pth")
vocab_tgt = torch.load("multi30k_processed_bpe/vocab_en.pth")

PAD_IDX = 1
BOS_IDX = 2
EOS_IDX = 3

SRC_VOCAB_SIZE = len(vocab_src)
TGT_VOCAB_SIZE = len(vocab_tgt)

# åŸè®ºæ–‡æ ‡å‡†é…ç½®
EMB_SIZE = 512
FFN_HID_DIM = 2048
NUM_ENCODER_LAYERS = 6
NUM_DECODER_LAYERS = 6
NUM_HEADS = 8
DROPOUT = 0.1

# æ–°å¢ä¼˜åŒ–å‚æ•°
NORM_FIRST = True  # Trueä¸ºPre-LayerNormï¼ŒFalseä¸ºPost-LayerNormï¼ˆåŸè®ºæ–‡ï¼‰
ACTIVATION = 'relu'  # 'relu' æˆ– 'gelu'
LABEL_SMOOTHING = 0.1  # Label smoothingå‚æ•°
WARMUP_STEPS = 4000  # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼ˆåŸè®ºæ–‡æ ‡å‡†ï¼‰

# è®­ç»ƒå‚æ•°ï¼ˆè€ƒè™‘æ˜¾å­˜é™åˆ¶çš„æ¢¯åº¦ç´¯ç§¯æ–¹æ¡ˆï¼‰
BATCH_SIZE = 8   # å‡å°å®é™…batch sizeé¿å…OOM
ACCUMULATE_GRAD_BATCHES = 4  # æ¢¯åº¦ç´¯ç§¯4æ­¥ï¼Œæœ‰æ•ˆbatch_size = 8*4 = 32ï¼ˆç­‰åŒåŸè®ºæ–‡ï¼‰
MAX_SEQ_LEN = 100  # é™åˆ¶åºåˆ—é•¿åº¦

# 2. æ•°æ®é›†å®šä¹‰
class ParallelTextDataset(Dataset):
    def __init__(self, src_path, tgt_path, max_samples=None):
        with open(src_path, encoding="utf-8") as f:
            self.src_lines = [line.strip() for line in f]
        with open(tgt_path, encoding="utf-8") as f:
            self.tgt_lines = [line.strip() for line in f]
        assert len(self.src_lines) == len(self.tgt_lines)
        if max_samples is not None:
            self.src_lines = self.src_lines[:max_samples]
            self.tgt_lines = self.tgt_lines[:max_samples]
    def __len__(self):
        return len(self.src_lines)
    def __getitem__(self, idx):
        return self.src_lines[idx], self.tgt_lines[idx]

def tokenize_bpe(text):
    return text.strip().split()

from torch.nn.utils.rnn import pad_sequence
def collate_fn(batch):
    src_batch, tgt_batch = [], []
    for src_sample, tgt_sample in batch:
        src_tokens = [BOS_IDX] + [vocab_src.get(tok, vocab_src.get('<unk>', 0)) for tok in tokenize_bpe(src_sample)] + [EOS_IDX]
        tgt_tokens = [BOS_IDX] + [vocab_tgt.get(tok, vocab_tgt.get('<unk>', 0)) for tok in tokenize_bpe(tgt_sample)] + [EOS_IDX]
        
        # é™åˆ¶åºåˆ—é•¿åº¦é˜²æ­¢OOM
        if len(src_tokens) > MAX_SEQ_LEN:
            src_tokens = src_tokens[:MAX_SEQ_LEN-1] + [EOS_IDX]
        if len(tgt_tokens) > MAX_SEQ_LEN:
            tgt_tokens = tgt_tokens[:MAX_SEQ_LEN-1] + [EOS_IDX]
            
        src_batch.append(torch.tensor(src_tokens, dtype=torch.long))
        tgt_batch.append(torch.tensor(tgt_tokens, dtype=torch.long))
    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)
    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)
    return src_batch, tgt_batch

train_dataset = ParallelTextDataset(
    "multi30k_processed_bpe/train_part1.tok.de",
    "multi30k_processed_bpe/train_part1.tok.en",
    max_samples=30000
)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)

val_dataset = ParallelTextDataset(
    "multi30k_processed_bpe/val.tok.de",
    "multi30k_processed_bpe/val.tok.en"
)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)

encoder = Encoder(SRC_VOCAB_SIZE, EMB_SIZE, NUM_HEADS, FFN_HID_DIM, NUM_ENCODER_LAYERS, 
                  DROPOUT, padding_idx=PAD_IDX, norm_first=NORM_FIRST, activation=ACTIVATION)
decoder = Decoder(TGT_VOCAB_SIZE, EMB_SIZE, NUM_HEADS, FFN_HID_DIM, NUM_DECODER_LAYERS, 
                  DROPOUT, padding_idx=PAD_IDX, norm_first=NORM_FIRST, activation=ACTIVATION)
class Seq2SeqTransformer(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
    def forward(self, src, tgt, src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask):
        memory = self.encoder(src, src_mask, src_key_padding_mask)
        output = self.decoder(tgt, memory, tgt_mask, tgt_key_padding_mask, src_key_padding_mask)
        return output

model = Seq2SeqTransformer(encoder, decoder)
# æ·»åŠ Label SmoothingæŸå¤±å‡½æ•°
class LabelSmoothingCrossEntropy(nn.Module):
    """
    Label SmoothingæŸå¤±å‡½æ•° - åŸè®ºæ–‡ä¸­æåˆ°çš„æ­£åˆ™åŒ–æŠ€æœ¯
    """
    def __init__(self, num_classes, smoothing=0.1, ignore_index=-100):
        super().__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.ignore_index = ignore_index
        self.confidence = 1.0 - smoothing
    
    def forward(self, pred, target):
        """
        pred: (N, C) logits
        target: (N,) targets
        """
        pred = pred.log_softmax(dim=-1)
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.num_classes - 1))
            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)
            true_dist.masked_fill_(target.unsqueeze(1) == self.ignore_index, 0)
        return torch.mean(torch.sum(-true_dist * pred, dim=-1))

# æ·»åŠ Warmupå­¦ä¹ ç‡è°ƒåº¦å™¨
class WarmupLRScheduler:
    """
    åŸè®ºæ–‡ä¸­çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥: lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))
    """
    def __init__(self, optimizer, d_model, warmup_steps=4000):
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.step_num = 0
    
    def step(self):
        self.step_num += 1
        lr = self.d_model ** (-0.5) * min(
            self.step_num ** (-0.5),
            self.step_num * self.warmup_steps ** (-1.5)
        )
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        return lr

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# ä½¿ç”¨Label SmoothingæŸå¤±å‡½æ•°
criterion = LabelSmoothingCrossEntropy(TGT_VOCAB_SIZE, smoothing=LABEL_SMOOTHING, ignore_index=PAD_IDX)

# ä½¿ç”¨åŸè®ºæ–‡æ¨èçš„Adamå‚æ•°
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)

# ä½¿ç”¨Warmupå­¦ä¹ ç‡è°ƒåº¦
warmup_scheduler = WarmupLRScheduler(optimizer, EMB_SIZE, WARMUP_STEPS)

def accuracy_fn(pred, target, pad_idx=PAD_IDX):
    pred = pred.argmax(-1)
    mask = target != pad_idx
    correct = (pred[mask] == target[mask]).sum().item()
    total = mask.sum().item()
    return correct, total

def greedy_decode(model, src, src_mask, src_pad_mask, max_len=50, bos_idx=BOS_IDX, eos_idx=EOS_IDX):
    # src: [1, src_len]
    memory = model.encoder(src, src_mask, src_pad_mask)
    ys = torch.ones(1, 1).fill_(bos_idx).type_as(src)
    for i in range(max_len-1):
        tgt_mask, _, _, tgt_pad_mask = create_mask(ys, ys, PAD_IDX)
        out = model.decoder(ys, memory, tgt_mask.to(src.device), tgt_pad_mask.to(src.device), src_pad_mask.to(src.device))
        prob = out[:, -1, :]
        next_word = prob.argmax(-1).unsqueeze(0)
        ys = torch.cat([ys, next_word], dim=1)
        if next_word.item() == eos_idx:
            break
    return ys.squeeze(0).tolist()[1:]  # å»æ‰BOS

def beam_search_decode(model, src, src_mask, src_pad_mask, beam_size=4, max_len=50, bos_idx=BOS_IDX, eos_idx=EOS_IDX):
    device = src.device
    memory = model.encoder(src, src_mask, src_pad_mask)
    ys = torch.ones(1, 1).fill_(bos_idx).long().to(device)
    sequences = [(ys, 0.0)]
    for _ in range(max_len - 1):
        all_candidates = []
        for seq, score in sequences:
            if seq[0, -1].item() == eos_idx:
                all_candidates.append((seq, score))
                continue
            tgt_mask, _, _, tgt_pad_mask = create_mask(seq, seq, PAD_IDX)
            out = model.decoder(seq, memory, tgt_mask.to(device), tgt_pad_mask.to(device), src_pad_mask.to(device))
            log_probs = torch.log_softmax(out[:, -1, :], dim=-1)
            topk_log_probs, topk_indices = log_probs.topk(beam_size, dim=-1)
            for k in range(beam_size):
                next_token = topk_indices[0, k].unsqueeze(0).unsqueeze(0)
                new_seq = torch.cat([seq, next_token], dim=1)
                new_score = score + topk_log_probs[0, k].item()
                all_candidates.append((new_seq, new_score))
        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)
        sequences = ordered[:beam_size]
        if all(seq[0][0, -1].item() == eos_idx for seq in sequences):
            break
    best_seq = sequences[0][0].squeeze(0).tolist()
    if bos_idx in best_seq:
        best_seq = best_seq[1:]
    if eos_idx in best_seq:
        best_seq = best_seq[:best_seq.index(eos_idx)]
    return best_seq

print("Using device:", device)
print("Model on:", next(model.parameters()).device)
def evaluate(model, data_loader, vocab_tgt, device, use_beam=True, beam_size=4):
    model.eval()
    total_loss = 0.0
    total_correct = 0
    total_tokens = 0
    refs = []
    hyps = []
    with torch.no_grad():
        for src, tgt in data_loader:
            src = src.to(device)
            tgt = tgt.to(device)
            tgt_input = tgt[:, :-1]
            tgt_output = tgt[:, 1:]
            src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input, PAD_IDX)
            logits = model(src, tgt_input, src_mask.to(device), tgt_mask.to(device), src_pad_mask.to(device), tgt_pad_mask.to(device))
            loss = criterion(logits.view(-1, logits.size(-1)), tgt_output.reshape(-1))
            total_loss += loss.item()
            correct, tokens = accuracy_fn(logits, tgt_output)
            total_correct += correct
            total_tokens += tokens
            # è§£ç BLEU
            for i in range(src.size(0)):
                src_sent = src[i].unsqueeze(0)
                src_mask_i = (src_sent != PAD_IDX).unsqueeze(1)
                src_pad_mask_i = (src_sent == PAD_IDX)
                if use_beam:
                    hyp = beam_search_decode(model, src_sent, src_mask_i, src_pad_mask_i, beam_size=beam_size)
                else:
                    hyp = greedy_decode(model, src_sent, src_mask_i, src_pad_mask_i)
                ref = tgt[i, 1:].tolist()
                if EOS_IDX in ref:
                    ref = ref[:ref.index(EOS_IDX)]
                if EOS_IDX in hyp:
                    hyp = hyp[:hyp.index(EOS_IDX)]
                refs.append([ref])
                hyps.append(hyp)
    avg_loss = total_loss / len(data_loader)
    acc = total_correct / total_tokens if total_tokens > 0 else 0
    bleu_score = corpus_bleu(refs, hyps)
    # ç¡®ä¿BLEUåˆ†æ•°æ˜¯floatç±»å‹
    if isinstance(bleu_score, (list, tuple)):
        bleu_score = float(bleu_score[0]) if len(bleu_score) > 0 else 0.0
    else:
        bleu_score = float(bleu_score)
    return avg_loss, acc, bleu_score

def train_one_epoch(model, data_loader, optimizer, warmup_scheduler, device, clip=1.0):
    model.train()
    total_loss = 0.0
    total_correct = 0
    total_tokens = 0
    current_lr = 0.0  # åˆå§‹åŒ–å˜é‡
    
    # æ¢¯åº¦ç´¯ç§¯ç›¸å…³å˜é‡
    accumulate_loss = 0.0
    
    for batch_idx, (src, tgt) in enumerate(data_loader):
        src = src.to(device)
        tgt = tgt.to(device)
        tgt_input = tgt[:, :-1]
        tgt_output = tgt[:, 1:]
        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input, PAD_IDX)
        
        with autocast():
            logits = model(src, tgt_input, src_mask.to(device), tgt_mask.to(device), src_pad_mask.to(device), tgt_pad_mask.to(device))
            loss = criterion(logits.view(-1, logits.size(-1)), tgt_output.reshape(-1))
            # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±è¦é™¤ä»¥ç´¯ç§¯æ­¥æ•°
            loss = loss / ACCUMULATE_GRAD_BATCHES
        
        scaler.scale(loss).backward()
        accumulate_loss += loss.item()
        
        # æ¯ACCUMULATE_GRAD_BATCHESæ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
        if (batch_idx + 1) % ACCUMULATE_GRAD_BATCHES == 0 or (batch_idx + 1) == len(data_loader):
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = warmup_scheduler.step()
        
        # ç»Ÿè®¡æ—¶è¦ä¹˜å›ç´¯ç§¯æ­¥æ•°ï¼Œä¿æŒlosså°ºåº¦ä¸€è‡´
        total_loss += accumulate_loss * ACCUMULATE_GRAD_BATCHES
        correct, tokens = accuracy_fn(logits, tgt_output)
        total_correct += correct
        total_tokens += tokens
        
        # é‡ç½®ç´¯ç§¯æŸå¤±
        if (batch_idx + 1) % ACCUMULATE_GRAD_BATCHES == 0:
            accumulate_loss = 0.0
        
    avg_loss = total_loss / len(data_loader)
    acc = total_correct / total_tokens if total_tokens > 0 else 0
    return avg_loss, acc, current_lr

# TensorBoardæ—¥å¿—
writer = SummaryWriter(log_dir="runs/transformer_exp")

# ...existing code...

num_epochs = 15  # å‡å°‘è®­ç»ƒè½®æ•°ï¼Œé¿å…è¿‡é•¿æ—¶é—´

# æ˜¾ç¤ºå½“å‰é…ç½®ä¿¡æ¯
print("="*60)
print("ğŸš€ Transformerè®­ç»ƒé…ç½® (ä¼˜åŒ–åé˜²æ­¢OOM)")
print("="*60)
print(f"ğŸ“Š æ•°æ®é…ç½®:")
print(f"   - è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset):,}")
print(f"   - éªŒè¯æ ·æœ¬æ•°: {len(val_dataset):,}")
print(f"   - å¾·è¯­è¯æ±‡é‡: {SRC_VOCAB_SIZE:,}")
print(f"   - è‹±è¯­è¯æ±‡é‡: {TGT_VOCAB_SIZE:,}")

print(f"\nğŸ—ï¸  æ¨¡å‹é…ç½® (å‡å°è§„æ¨¡é˜²æ­¢OOM):")
print(f"   - åµŒå…¥ç»´åº¦: {EMB_SIZE}")
print(f"   - FFNéšè—å±‚: {FFN_HID_DIM}")
print(f"   - ç¼–ç å™¨å±‚æ•°: {NUM_ENCODER_LAYERS}")
print(f"   - è§£ç å™¨å±‚æ•°: {NUM_DECODER_LAYERS}")
print(f"   - æ³¨æ„åŠ›å¤´æ•°: {NUM_HEADS}")
print(f"   - Dropout: {DROPOUT}")

print(f"\nâš™ï¸  è®­ç»ƒé…ç½®:")
print(f"   - Batch Size: {BATCH_SIZE} (å‡å°é˜²æ­¢OOM)")
print(f"   - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {ACCUMULATE_GRAD_BATCHES}")
print(f"   - æœ‰æ•ˆBatch Size: {BATCH_SIZE * ACCUMULATE_GRAD_BATCHES}")
print(f"   - å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}")
print(f"   - Warmupæ­¥æ•°: {WARMUP_STEPS}")
print(f"   - è®­ç»ƒè½®æ•°: {num_epochs}")
print(f"   - Label Smoothing: {LABEL_SMOOTHING}")

print(f"\nğŸ’» ç¡¬ä»¶é…ç½®:")
print(f"   - è®¾å¤‡: {device}")
if device.type == 'cuda':
    print(f"   - GPUåç§°: {torch.cuda.get_device_name()}")
    print(f"   - æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"   - æ··åˆç²¾åº¦: å¯ç”¨ (AMP)")

print("="*60)
print("å¼€å§‹è®­ç»ƒ...")
print("="*60)

best_bleu = 0.0
beam_eval_interval = 5  # æ¯Nè½®ç”¨beam searchè¯„ä¼°ä¸€æ¬¡

for epoch in range(num_epochs):
    train_loss, train_acc, current_lr = train_one_epoch(model, train_loader, optimizer, warmup_scheduler, device)
    # è®­ç»ƒæ—¶åªç”¨greedyè§£ç è¯„ä¼°
    val_loss, val_acc, val_bleu = evaluate(model, val_loader, vocab_tgt, device, use_beam=False)
    
    print(f"Epoch {epoch+1}:")
    print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, LR: {current_lr:.6f}")
    print(f"  Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}, BLEU (greedy): {val_bleu:.4f}")
    writer.add_scalar("Loss/train", train_loss, epoch+1)
    writer.add_scalar("Loss/val", val_loss, epoch+1)
    writer.add_scalar("Accuracy/train", train_acc, epoch+1)
    writer.add_scalar("Accuracy/val", val_acc, epoch+1)
    writer.add_scalar("BLEU/val_greedy", val_bleu, epoch+1)
    writer.add_scalar("Learning_Rate", current_lr, epoch+1)
    
    # æ¯Nè½®ç”¨beam searchè¯„ä¼°BLEU
    if (epoch + 1) % beam_eval_interval == 0 or (epoch + 1) == num_epochs:
        val_loss_beam, val_acc_beam, val_bleu_beam = evaluate(model, val_loader, vocab_tgt, device, use_beam=True, beam_size=4)
        print(f"  Val BLEU (beam search): {val_bleu_beam:.4f}")
        writer.add_scalar("BLEU/val_beam", val_bleu_beam, epoch+1)
        # ä¿å­˜æœ€ä½³beam BLEUæ¨¡å‹
        if val_bleu_beam > best_bleu:
            best_bleu = val_bleu_beam
            torch.save(model.state_dict(), "best_transformer_model.pth")
            print(f"  Best model saved at epoch {epoch+1} (BLEU={val_bleu_beam:.4f})")
# æœ€ç»ˆæ¨¡å‹
torch.save(model.state_dict(), "transformer_model.pth")
writer.close()
print("è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å‚æ•°å·²ä¿å­˜è‡³ transformer_model.pth")