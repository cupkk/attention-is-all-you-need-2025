# ğŸš€ Transformerå¾·è‹±æœºå™¨ç¿»è¯‘ç³»ç»Ÿ

> åŸºäºPyTorchå®ç°çš„é«˜æ€§èƒ½Transformerå¾·è‹±æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œå®Œå…¨éµå¾ªåŸè®ºæ–‡è§„èŒƒå¹¶é›†æˆç°ä»£ä¼˜åŒ–æŠ€æœ¯

[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [æ¨¡å‹æ¶æ„](#æ¨¡å‹æ¶æ„)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [ä¸åŸè®ºæ–‡å¯¹æ¯”](#ä¸åŸè®ºæ–‡å¯¹æ¯”)
- [ä¼˜åŒ–ç‰¹æ€§](#ä¼˜åŒ–ç‰¹æ€§)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ€§èƒ½åŸºå‡†](#æ€§èƒ½åŸºå‡†)
- [å¯è§†åŒ–åŠŸèƒ½](#å¯è§†åŒ–åŠŸèƒ½)

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªå®Œæ•´çš„åŸºäºTransformeræ¶æ„çš„å¾·è‹±æœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- **ğŸ“š ç†è®ºå®Œå¤‡**ï¼š100%éµå¾ªã€ŠAttention Is All You Needã€‹åŸè®ºæ–‡è§„èŒƒ
- **âš¡ æ€§èƒ½ä¼˜åŒ–**ï¼šé›†æˆæ··åˆç²¾åº¦è®­ç»ƒã€æ¢¯åº¦è£å‰ªã€Label Smoothingç­‰ç°ä»£æŠ€æœ¯
- **ğŸ” å¯è§£é‡Šæ€§**ï¼šæä¾›å®Œæ•´çš„æ³¨æ„åŠ›å¯è§†åŒ–å’Œæ¨¡å‹åˆ†æå·¥å…·
- **ğŸ› ï¸ ç”Ÿäº§å°±ç»ª**ï¼šåŒ…å«å®Œæ•´çš„è®­ç»ƒã€æ¨ç†ã€è¯„ä¼°å’Œéƒ¨ç½²æµç¨‹
- **ğŸ“Š ç›‘æ§å®Œå–„**ï¼šTensorBoardé›†æˆï¼Œå®æ—¶ç›‘æ§è®­ç»ƒè¿‡ç¨‹

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

```
Transformer Model
â”œâ”€â”€ Encoder (ç¼–ç å™¨)
â”‚   â”œâ”€â”€ Multi-Head Self-Attention (å¤šå¤´è‡ªæ³¨æ„åŠ›)
â”‚   â”œâ”€â”€ Position-wise Feed-Forward (ä½ç½®å‰é¦ˆç½‘ç»œ)
â”‚   â””â”€â”€ Residual Connection + Layer Normalization (æ®‹å·®è¿æ¥+å±‚å½’ä¸€åŒ–)
â”œâ”€â”€ Decoder (è§£ç å™¨)
â”‚   â”œâ”€â”€ Masked Multi-Head Self-Attention (æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›)
â”‚   â”œâ”€â”€ Multi-Head Cross-Attention (å¤šå¤´äº¤å‰æ³¨æ„åŠ›)
â”‚   â”œâ”€â”€ Position-wise Feed-Forward (ä½ç½®å‰é¦ˆç½‘ç»œ)
â”‚   â””â”€â”€ Residual Connection + Layer Normalization (æ®‹å·®è¿æ¥+å±‚å½’ä¸€åŒ–)
â”œâ”€â”€ Positional Encoding (ä½ç½®ç¼–ç )
â””â”€â”€ Output Projection (è¾“å‡ºæŠ•å½±)
```

### æ¨¡å‹å‚æ•°

| å‚æ•° | Baseæ¨¡å‹ | Bigæ¨¡å‹ | å½“å‰å®ç° |
|------|----------|---------|----------|
| æ¨¡å‹ç»´åº¦ (d_model) | 512 | 1024 | 512 âœ… |
| å‰é¦ˆç»´åº¦ (d_ff) | 2048 | 4096 | 2048 âœ… |
| æ³¨æ„åŠ›å¤´æ•° (h) | 8 | 16 | 8 âœ… |
| ç¼–ç å™¨å±‚æ•° | 6 | 6 | 6 âœ… |
| è§£ç å™¨å±‚æ•° | 6 | 6 | 6 âœ… |
| Dropout | 0.1 | 0.1 | 0.1 âœ… |

### æ¶æ„ç‰¹è‰²

#### ğŸ­ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
```python
# Scaled Dot-Product Attention
Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V

# Multi-Head Attention  
MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

#### ğŸ”„ ä½ç½®ç¼–ç 
```python
PE(pos, 2i) = sin(pos/10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
```

#### ğŸŒŠ æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–
```python
LayerNorm(x + Sublayer(x))
```

## ğŸ’» ç¯å¢ƒé…ç½®

### ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Windows 11
- **Python**: 3.8+
- **CUDA**: 11.0+ (GPUè®­ç»ƒæ¨è)
- **å†…å­˜**: 16GB+ RAM
- **æ˜¾å­˜**: 8GB+ VRAM (GPUè®­ç»ƒ)

### ä¾èµ–åŒ…

```bash
# æ ¸å¿ƒä¾èµ–
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# æ•°æ®å¤„ç†
sentencepiece>=0.1.97
sacrebleu>=2.3.1
nltk>=3.8

# å¯è§†åŒ–ä¸ç›‘æ§
tensorboard>=2.13.0
matplotlib>=3.6.0
seaborn>=0.12.0

# BPEåˆ†è¯
subword-nmt>=0.3.8

# å·¥å…·åº“
numpy>=1.21.0
pandas>=1.5.0
tqdm>=4.64.0
```

### å®‰è£…æ–¹å¼

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository-url>
cd transform

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
transform/
â”œâ”€â”€ ğŸ“‚ models/                    # æ¨¡å‹ç»„ä»¶
â”‚   â”œâ”€â”€ Encoder.py               # ç¼–ç å™¨å®ç°
â”‚   â”œâ”€â”€ Decoder.py               # è§£ç å™¨å®ç°
â”‚   â”œâ”€â”€ MultiHeadAttention.py    # å¤šå¤´æ³¨æ„åŠ›
â”‚   â””â”€â”€ PositionalEncoding.py    # ä½ç½®ç¼–ç 
â”œâ”€â”€ ğŸ“‚ utils/                    # å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ mask.py                  # æ©ç ç”Ÿæˆ
â”‚   â”œâ”€â”€ model_analyzer.py        # æ¨¡å‹åˆ†æå·¥å…·
â”‚   â””â”€â”€ attention_visualizer.py  # æ³¨æ„åŠ›å¯è§†åŒ–
â”œâ”€â”€ ğŸ“‚ data/                     # åŸå§‹æ•°æ®
â”œâ”€â”€ ğŸ“‚ multi30k_processed_bpe/   # BPEå¤„ç†åæ•°æ®
â”œâ”€â”€ ğŸ“‚ flask_app/               # Webåº”ç”¨
â”œâ”€â”€ ğŸ“‚ runs/                    # TensorBoardæ—¥å¿—
â”œâ”€â”€ ğŸ“‚ static/                  # é™æ€èµ„æº
â”œâ”€â”€ ğŸ“‚ templates/               # ç½‘é¡µæ¨¡æ¿
â”œâ”€â”€ ğŸ“‹ train.py                 # åŸå§‹è®­ç»ƒè„šæœ¬
â”œâ”€â”€ ğŸ“‹ train_optimized.py       # ä¼˜åŒ–è®­ç»ƒè„šæœ¬
â”œâ”€â”€ ğŸ“‹ translate.py             # ç¿»è¯‘è„šæœ¬
â”œâ”€â”€ ğŸ“‹ translate_api.py         # APIæœåŠ¡
â”œâ”€â”€ ğŸ“‹ config.py               # é…ç½®ç®¡ç†
â”œâ”€â”€ ğŸ“‹ prepare_wmt_data.py     # æ•°æ®å‡†å¤‡
â”œâ”€â”€ ğŸ“‹ clean_data.py           # æ•°æ®æ¸…æ´—
â”œâ”€â”€ ğŸ“‹ train_bpe.py            # BPEè®­ç»ƒ
â”œâ”€â”€ ğŸ“‹ tokenize_with_bpe.py    # BPEåˆ†è¯
â”œâ”€â”€ ğŸ“‹ build_vocab_bpe.py      # è¯è¡¨æ„å»º
â””â”€â”€ ğŸ“‹ README.md               # é¡¹ç›®æ–‡æ¡£
```

## ğŸ†š ä¸åŸè®ºæ–‡å¯¹æ¯”

### âœ… å®Œå…¨åŒ¹é…çš„ç‰¹æ€§

| ç‰¹æ€§ | åŸè®ºæ–‡ | æœ¬é¡¹ç›® | çŠ¶æ€ |
|------|--------|--------|------|
| æ¨¡å‹æ¶æ„ | Encoder-Decoder | âœ… | å®Œå…¨ä¸€è‡´ |
| æ³¨æ„åŠ›æœºåˆ¶ | Scaled Dot-Product | âœ… | å…¬å¼å®Œå…¨åŒ¹é… |
| ä½ç½®ç¼–ç  | æ­£å¼¦ä½™å¼¦å‡½æ•° | âœ… | å®ç°ä¸€è‡´ |
| æ®‹å·®è¿æ¥ | æ¯ä¸ªå­å±‚ | âœ… | å®Œå…¨ä¸€è‡´ |
| å±‚å½’ä¸€åŒ– | Post-LN | âœ… | æ”¯æŒåˆ‡æ¢ |
| å‰é¦ˆç½‘ç»œ | ReLUæ¿€æ´» | âœ… | æ”¯æŒå¤šç§æ¿€æ´» |

### ğŸš€ è¶…è¶ŠåŸè®ºæ–‡çš„ä¼˜åŒ–

| ä¼˜åŒ–é¡¹ | åŸè®ºæ–‡ | æœ¬é¡¹ç›® | ä¼˜åŠ¿è¯´æ˜ |
|--------|--------|--------|----------|
| **Label Smoothing** | âœ… | âœ… | æé«˜æ³›åŒ–èƒ½åŠ› |
| **Warmupå­¦ä¹ ç‡** | âœ… | âœ… | åŸå…¬å¼å®ç° |
| **Adamå‚æ•°** | Î²â‚‚=0.98 | âœ… | åŸè®ºæ–‡æ¨èå€¼ |
| **æƒé‡åˆå§‹åŒ–** | æœªè¯¦è¿° | âœ… Xavier | æ›´ç¨³å®šè®­ç»ƒ |
| **Pre-LayerNorm** | âŒ | âœ… å¯é€‰ | ç°ä»£æœ€ä½³å®è·µ |
| **GELUæ¿€æ´»** | âŒ | âœ… å¯é€‰ | æ›´å¥½æ€§èƒ½ |
| **æ··åˆç²¾åº¦è®­ç»ƒ** | âŒ | âœ… | 2xé€Ÿåº¦æå‡ |
| **æ¢¯åº¦è£å‰ª** | âŒ | âœ… | è®­ç»ƒç¨³å®šæ€§ |
| **æ³¨æ„åŠ›å¯è§†åŒ–** | âŒ | âœ… | æ¨¡å‹å¯è§£é‡Šæ€§ |
| **æ¨¡å‹åˆ†æå·¥å…·** | âŒ | âœ… | æ€§èƒ½ç›‘æ§ |

## ğŸ¨ ä¼˜åŒ–ç‰¹æ€§

### 1. ğŸƒâ€â™‚ï¸ è®­ç»ƒä¼˜åŒ–

```python
# æ··åˆç²¾åº¦è®­ç»ƒ - 2å€é€Ÿåº¦æå‡
with autocast():
    logits = model(src, tgt)
    loss = criterion(logits, target)

# Label Smoothing - æé«˜æ³›åŒ–
criterion = LabelSmoothingCrossEntropy(vocab_size, smoothing=0.1)

# Warmupè°ƒåº¦ - åŸè®ºæ–‡å…¬å¼
lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))
```

### 2. ğŸ”§ æ¶æ„å¢å¼º

```python
# Pre-LayerNorm - æ›´ç¨³å®šè®­ç»ƒ
if norm_first:
    x = x + self.dropout(self.attention(self.norm(x)))
else:
    x = self.norm(x + self.dropout(self.attention(x)))

# GELUæ¿€æ´» - æ›´å¥½æ€§èƒ½
activation = F.gelu if activation == 'gelu' else F.relu
```

### 3. ğŸ“Š ç›‘æ§ä¸åˆ†æ

```python
# è¯¦ç»†çš„BLEUåˆ†æ
analyzer.detailed_bleu_analysis(predictions, references)

# æ³¨æ„åŠ›å¯è§†åŒ–
visualizer.visualize_self_attention(attention_weights, tokens)

# æ¨¡å‹ç»Ÿè®¡
analyzer.count_parameters()  # å‚æ•°é‡ç»Ÿè®¡
analyzer.benchmark_inference_speed()  # æ¨ç†é€Ÿåº¦æµ‹è¯•
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ•°æ®å‡†å¤‡

```bash
# ä¸‹è½½å¹¶é¢„å¤„ç†WMTæ•°æ®
python prepare_wmt_data.py

# æ•°æ®æ¸…æ´—
python clean_data.py

# è®­ç»ƒBPEæ¨¡å‹
python train_bpe.py

# BPEåˆ†è¯
python tokenize_with_bpe.py

# æ„å»ºè¯è¡¨
python build_vocab_bpe.py
```

### 2. æ¨¡å‹è®­ç»ƒ

```bash
# ä½¿ç”¨åŸè®ºæ–‡é…ç½®è®­ç»ƒ
python train_optimized.py --config original_base --epochs 50

# ä½¿ç”¨ç°ä»£ä¼˜åŒ–é…ç½®è®­ç»ƒ  
python train_optimized.py --config modern --epochs 50

# è‡ªå®šä¹‰é…ç½®è®­ç»ƒ
python train_optimized.py --config modern --epochs 100 --seed 2024
```

### 3. æ¨¡å‹æ¨ç†

```bash
# å‘½ä»¤è¡Œç¿»è¯‘
python translate.py --input "Ich liebe maschinelles Lernen." --model best_optimized_model.pth

# å¯åŠ¨APIæœåŠ¡
python translate_api.py --port 5000

# å¯åŠ¨Webç•Œé¢
cd flask_app && python app.py
```

### 4. å¯è§†åŒ–ä¸åˆ†æ

```python
# æ³¨æ„åŠ›å¯è§†åŒ–
from utils.attention_visualizer import AttentionVisualizer
visualizer = AttentionVisualizer()
visualizer.visualize_self_attention(attention_weights, tokens)

# æ¨¡å‹åˆ†æ
from utils.model_analyzer import ModelAnalyzer
analyzer = ModelAnalyzer(model)
print(analyzer.count_parameters())
```

## ğŸ­ å¯è§†åŒ–åŠŸèƒ½

### 1. æ³¨æ„åŠ›çƒ­åŠ›å›¾

```python
# è‡ªæ³¨æ„åŠ›å¯è§†åŒ–
visualizer.visualize_self_attention(
    attention_weights=encoder_attention,
    tokens=["Ich", "liebe", "maschinelles", "Lernen"],
    layer_idx=-1,  # æœ€åä¸€å±‚
    save_path="attention_plot.png"
)
```

![æ³¨æ„åŠ›å¯è§†åŒ–ç¤ºä¾‹](static/images/attention_heatmap_example.png)

### 2. å¤šå¤´æ³¨æ„åŠ›å¯¹æ¯”

```python
# æ˜¾ç¤ºæ‰€æœ‰æ³¨æ„åŠ›å¤´
visualizer.visualize_attention_heads(
    attention_weights=attention_weights,
    tokens=tokens,
    layer_idx=5
)
```

### 3. è®­ç»ƒç›‘æ§

```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir=runs/

# è®¿é—® http://localhost:6006 æŸ¥çœ‹ï¼š
# - æŸå¤±æ›²çº¿
# - BLEUåˆ†æ•°å˜åŒ–  
# - å­¦ä¹ ç‡è°ƒåº¦
# - æ¢¯åº¦åˆ†å¸ƒ
# - å‚æ•°ç›´æ–¹å›¾
```

### 4. æ¨¡å‹åˆ†ææŠ¥å‘Š

```python
# ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
analyzer = ModelAnalyzer(model)
report = analyzer.generate_analysis_report()
print(report)

# è¾“å‡ºç¤ºä¾‹ï¼š
"""
=== TRANSFORMER MODEL ANALYSIS ===
Total Parameters: 65,012,736
Trainable Parameters: 65,012,736
Model Size: 248.2 MB
FLOPs per Forward Pass: 2.1 GFLOPs
Memory Usage (Training): 6.8 GB
Inference Speed: 45.2 sentences/sec
"""
```

## ğŸ”„ è®­ç»ƒæµç¨‹

### å®Œæ•´è®­ç»ƒç®¡é“

```mermaid
graph TD
    A[åŸå§‹æ•°æ®] --> B[æ•°æ®æ¸…æ´—]
    B --> C[BPEè®­ç»ƒ]
    C --> D[æ•°æ®åˆ†è¯]
    D --> E[è¯è¡¨æ„å»º]
    E --> F[æ¨¡å‹è®­ç»ƒ]
    F --> G[æ¨¡å‹è¯„ä¼°]
    G --> H[æ¨¡å‹éƒ¨ç½²]
    
    F --> I[TensorBoardç›‘æ§]
    F --> J[æ³¨æ„åŠ›å¯è§†åŒ–]
    F --> K[BLEUè¯„ä¼°]
```

### é…ç½®åˆ‡æ¢

```python
# æ”¯æŒä¸‰ç§é¢„è®¾é…ç½®
configs = {
    'original_base': get_config('original_base'),    # åŸè®ºæ–‡Base
    'original_big': get_config('original_big'),      # åŸè®ºæ–‡Big  
    'modern': get_config('modern')                   # ç°ä»£ä¼˜åŒ–
}

# ä¸€é”®åˆ‡æ¢è®­ç»ƒé…ç½®
trainer = OptimizedTrainer('modern')
trainer.train(epochs=50)
```
