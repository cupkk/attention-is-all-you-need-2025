import pandas as pd
import os
import re
from hashlib import md5
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def clean_text(text, lowercase=True, max_length=50, multilingual=False):
    """
    æ¸…æ´—å•å¥æ–‡æœ¬ï¼Œå¹¶ç»Ÿè®¡è¢«è¿‡æ»¤çš„å¥å­æ•°é‡
    
    å‚æ•°:
        text (str): åŸå§‹å¥å­
        lowercase (bool): æ˜¯å¦è½¬ä¸ºå°å†™
        max_length (int): æœ€å¤§ token é•¿åº¦
        multilingual (bool): æ˜¯å¦å¯ç”¨å¤šè¯­è¨€æ”¯æŒ
    
    è¿”å›:
        str: æ¸…æ´—åçš„å¥å­æˆ– Noneï¼ˆå¦‚æœè¶…è¿‡é•¿åº¦ï¼‰
    """

    if not isinstance(text, str) or not text.strip():
        return None
    

    # å»é™¤ HTML æ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)

    # ä¿ç•™æ›´å¤šç¬¦å·ï¼ˆåŒ…æ‹¬æ‹¬å·ç±»å’Œæ•°å­¦ç¬¦å·ï¼‰
    if multilingual:
        pattern = r'[^\w\s.,!?;:\'"$â‚¬Â£Â¥Â¢Â¡Â¿Â¿Â«Â»ã€Šã€‹â€¦+\-Ã—Ã·\u0080-\U0010ffff]'
    else:
        pattern = r'[^\w\s.,!?;:\'"$â‚¬Â£Â¥Â¢Â¡Â¿+\-Ã—Ã·]'
        
    text = re.sub(pattern, '', text)
    
    # è§„èŒƒç©ºç™½ç¬¦
    text = ' '.join(text.strip().split())
    
    # å°å†™è½¬æ¢
    if lowercase:
        text = text.lower()
        
    # æ§åˆ¶æœ€å¤§é•¿åº¦
    if max_length and len(text.split()) > max_length:
        return None
    
    return text


def process_parquet_file(parquet_path, output_de, output_en, 
                        max_length=50, lowercase=True, multilingual=False):
    """
    å¤„ç† .parquet æ–‡ä»¶ï¼Œæå– de/en å¥å­å¹¶æ¸…æ´—ä¿å­˜ä¸º .de/.en æ–‡ä»¶
    
    å‚æ•°:
        parquet_path: è¾“å…¥ Parquet æ–‡ä»¶è·¯å¾„
        output_de: è¾“å‡ºå¾·è¯­æ–‡ä»¶è·¯å¾„
        output_en: è¾“å‡ºè‹±è¯­æ–‡ä»¶è·¯å¾„
        max_length: æœ€å¤§å¥å­é•¿åº¦
        lowercase: æ˜¯å¦ç»Ÿä¸€è½¬ä¸ºå°å†™
        multilingual: æ˜¯å¦å¯ç”¨å¤šè¯­è¨€æ”¯æŒ
    """
    logger.info(f"ğŸ“– æ­£åœ¨å¤„ç†: {parquet_path}")
    
    try:
        df = pd.read_parquet(parquet_path)
    except Exception as e:
        logger.error(f"è¯»å–å¤±è´¥: {parquet_path} - {str(e)}")
        return
        
    # è‡ªåŠ¨æ£€æµ‹ç¿»è¯‘å­—æ®µ
    if 'translation' in df.columns:
        de_sentences = [item['de'] if isinstance(item, dict) else '' for item in df['translation']]
        en_sentences = [item['en'] if isinstance(item, dict) else '' for item in df['translation']]
    else:
        logger.error(f"âŒ ç¼ºå¤± translation å­—æ®µ: {parquet_path}")
        return

    logger.info(f"å…±è¯»å– {len(de_sentences)} å¯¹å¥å­")
    logger.info(f"å¼€å§‹æ¸…æ´—ï¼Œæœ€å¤§é•¿åº¦: {max_length}, å°å†™åŒ–: {lowercase}, å¤šè¯­è¨€: {multilingual}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_de).parent.mkdir(parents=True, exist_ok=True)
    Path(output_en).parent.mkdir(parents=True, exist_ok=True)

    counter = {'total': 0, 'too_long': 0, 'valid': 0}
    seen_hashes = set()

    with open(output_de, "w", encoding="utf-8") as f_de, \
         open(output_en, "w", encoding="utf-8") as f_en:

        for de_line, en_line in zip(de_sentences, en_sentences):
            counter['total'] += 1
            
            # æ¸…æ´—æ–‡æœ¬
            de_cleaned = clean_text(de_line, lowercase=lowercase, 
                                 max_length=max_length, multilingual=multilingual)
            en_cleaned = clean_text(en_line, lowercase=lowercase, 
                                 max_length=max_length, multilingual=multilingual)
            
            # ç©ºå¥æˆ–æ¸…æ´—å¤±è´¥
            if not de_cleaned or not en_cleaned:
                counter['too_long'] += 1
                continue
                
            # å»é‡é€»è¾‘
            pair_hash = md5(f"{de_cleaned}|||{en_cleaned}".encode()).hexdigest()
            if pair_hash in seen_hashes:
                continue
            seen_hashes.add(pair_hash)
            
            # å†™å…¥æ–‡ä»¶
            f_de.write(de_cleaned + "\n")
            f_en.write(en_cleaned + "\n")
            counter['valid'] += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if counter['total'] % 10000 == 0:
                logger.debug(f"å·²å¤„ç† {counter['total']} å¥ï¼Œæœ‰æ•ˆ {counter['valid']} å¥")

    logger.info(f"âœ… æ¸…æ´—å®Œæˆ: {parquet_path} -> {output_de} & {output_en}")
    logger.info(f"ğŸ“Š ç»Ÿè®¡: æ€»={counter['total']} æœ‰æ•ˆ={counter['valid']} è¿‡é•¿={counter['too_long']}")


if __name__ == "__main__":
    data_dir = "data"
    output_dir = "data_dir"
    
    # è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰ Parquet æ–‡ä»¶
    import glob
    parquet_files = glob.glob(os.path.join(data_dir, "*.parquet"))
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # å¤„ç†è®­ç»ƒé›†ï¼ˆä»…å¤„ç† train_part1ï¼‰
    for i in range(1, 2):  # ä»…å¤„ç† train_part1
        train_parquet = os.path.join(data_dir, f"train_part{i}.parquet")
        if not os.path.exists(train_parquet):
            logger.warning(f"âš ï¸ è·³è¿‡ç¼ºå¤±æ–‡ä»¶: {train_parquet}")
            continue
            
        process_parquet_file(
            train_parquet,
            os.path.join(output_dir, f"train_part{i}.de"),
            os.path.join(output_dir, f"train_part{i}.en"),
            max_length=50,
            lowercase=True,
            multilingual=True
        )
    
    # å¤„ç†éªŒè¯é›†
    val_parquet = os.path.join(data_dir, "validation.parquet")
    if os.path.exists(val_parquet):
        process_parquet_file(
            val_parquet,
            os.path.join(output_dir, "validation.de"),
            os.path.join(output_dir, "validation.en"),
            max_length=50,
            lowercase=True,
            multilingual=True
        )
    
    # å¤„ç†æµ‹è¯•é›†ï¼ˆç¡®ä¿æ¸…æ´—é€»è¾‘ä¸€è‡´ï¼‰
    test_parquet = os.path.join(data_dir, "test.parquet")
    if os.path.exists(test_parquet):
        process_parquet_file(
            test_parquet,
            os.path.join(output_dir, "test.de"),
            os.path.join(output_dir, "test.en"),
            max_length=50,
            lowercase=True,
            multilingual=True
        )
